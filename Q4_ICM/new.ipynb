{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic Curiosity Module (ICM)\n",
    "class StateEncoder(nn.Module):\n",
    "    def __init__(self, state_dim, encoding_dim):\n",
    "        super(StateEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, encoding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.encoder(state)\n",
    "\n",
    "class InverseModel(nn.Module):\n",
    "    def __init__(self, encoding_dim, action_dim):\n",
    "        super(InverseModel, self).__init__()\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(encoding_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, state_encoding, next_state_encoding):\n",
    "        input_tensor = torch.cat([state_encoding, next_state_encoding], dim=0)\n",
    "        return self.inverse_model(input_tensor)\n",
    "\n",
    "class ForwardModel(nn.Module):\n",
    "    def __init__(self, encoding_dim, action_dim, stack_size=4):\n",
    "        super(ForwardModel, self).__init__()\n",
    "        self.stack_size = stack_size\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(encoding_dim * stack_size + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, encoding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, state_encoding_stack, action):\n",
    "        input_tensor = torch.cat([state_encoding_stack.view(-1, self.stack_size * state_encoding_stack.shape[-1]), action], dim=1)\n",
    "        return self.forward_model(input_tensor)\n",
    "\n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, encoding_dim, stack_size=4):\n",
    "        super(ICM, self).__init__()\n",
    "        self.state_encoder = StateEncoder(state_dim, encoding_dim)\n",
    "        self.inverse_model = InverseModel(encoding_dim, action_dim)\n",
    "        self.forward_model = ForwardModel(encoding_dim, action_dim, stack_size)\n",
    "\n",
    "    def forward(self, state, next_state, action):\n",
    "        state_encoding = self.state_encoder(state)\n",
    "        next_state_encoding = self.state_encoder(next_state)\n",
    "\n",
    "        inverse_loss = nn.MSELoss()(self.inverse_model(state_encoding, next_state_encoding), action)\n",
    "\n",
    "        state_encoding_stack = torch.cat([state_encoding.unsqueeze(1)] * self.forward_model.stack_size, dim=1)\n",
    "        forward_loss = nn.MSELoss()(self.forward_model(state_encoding_stack, action), next_state_encoding)\n",
    "\n",
    "        return inverse_loss, forward_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Agent\n",
    "def train_agent(env, episodes, batch_size, buffer_size, gamma, epsilon, epsilon_decay):\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    # action_dim = env.action_space.shape[0]\n",
    "    action_dim = 3\n",
    "    encoding_dim = 64\n",
    "    stack_size = 4\n",
    "\n",
    "    dqn = DQN(state_dim, action_dim).to(device)\n",
    "    icm = ICM(state_dim, action_dim, encoding_dim, stack_size).to(device)\n",
    "    dqn_optimizer = optim.Adam(dqn.parameters())\n",
    "    icm_optimizer = optim.Adam(icm.parameters())\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Choose action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                q_values = dqn(state_tensor)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "            # Take action and observe next state\n",
    "            print(action)\n",
    "            next_state, reward, done = env.step(action)[:3]\n",
    "\n",
    "            print(next_state, reward, done)\n",
    "            print (state)\n",
    "            \n",
    "            #convert state from tuple to pytorch tensor>\n",
    "            state = np.array(state[0])\n",
    "            print(state.shape)\n",
    "            \n",
    "            # Compute intrinsic reward\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).reshape(1, -1)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32, device=device).reshape(1, -1)\n",
    "            action_tensor = torch.tensor([action], dtype=torch.float32, device=device)\n",
    "\n",
    "            inverse_loss, forward_loss = icm(state_tensor.squeeze(0), next_state_tensor.squeeze(0), action_tensor)\n",
    "            intrinsic_reward = forward_loss.item()\n",
    "\n",
    "            # Add experience to replay buffer\n",
    "            replay_buffer.append(state, action, reward + intrinsic_reward, next_state, done)\n",
    "\n",
    "            # Sample from replay buffer and update networks\n",
    "            if len(replay_buffer.buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                states_tensor = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "                actions_tensor = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "                rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "                next_states_tensor = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "                dones_tensor = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "                # Update DQN\n",
    "                q_values = dqn(states_tensor).gather(1, actions_tensor)\n",
    "                next_q_values = dqn(next_states_tensor).max(1)[0].detach()\n",
    "                expected_q_values = rewards_tensor + gamma * next_q_values * (1 - dones_tensor)\n",
    "                dqn_loss = nn.MSELoss()(q_values, expected_q_values)\n",
    "\n",
    "                dqn_optimizer.zero_grad()\n",
    "                dqn_loss.backward()\n",
    "                dqn_optimizer.step()\n",
    "\n",
    "                # Update ICM\n",
    "                icm_optimizer.zero_grad()\n",
    "                inverse_loss, forward_loss = icm(states_tensor, next_states_tensor, actions_tensor)\n",
    "                icm_loss = inverse_loss + forward_loss\n",
    "                icm_loss.backward()\n",
    "                icm_optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        epsilon *= epsilon_decay\n",
    "        print(f\"Episode {episode + 1}, Reward: {episode_reward}, Epsilon: {epsilon}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5689214  -0.3068337  -0.31598258]\n",
      "[ 1.25290218e+00  1.57711218e-03 -1.15017681e-03 -2.00021029e-03\n",
      " -5.16223835e-03 -1.54648040e-03 -6.70833983e-02  5.18844074e-01\n",
      "  8.73329144e-01 -4.60344515e-01 -4.47067941e-01] 0.9995998568170228 False\n",
      "(array([ 1.25317181e+00, -5.10535800e-04, -4.63249702e-03, -1.56694872e-04,\n",
      "       -3.36991884e-03,  1.78174941e-03, -4.99624155e-04,  2.37616947e-03,\n",
      "       -3.87319959e-03,  4.36671896e-05, -5.09108664e-04]), {})\n",
      "(11,)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 16 but got size 1 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     10\u001b[0m epsilon_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_decay\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 45\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, episodes, batch_size, buffer_size, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     42\u001b[0m next_state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m action_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([action], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 45\u001b[0m inverse_loss, forward_loss \u001b[38;5;241m=\u001b[39m \u001b[43micm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m intrinsic_reward \u001b[38;5;241m=\u001b[39m forward_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Add experience to replay buffer\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 55\u001b[0m, in \u001b[0;36mICM.forward\u001b[0;34m(self, state, next_state, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m inverse_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minverse_model(state_encoding, next_state_encoding), action)\n\u001b[1;32m     54\u001b[0m state_encoding_stack \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state_encoding\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_model\u001b[38;5;241m.\u001b[39mstack_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m forward_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_encoding_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m, next_state_encoding)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inverse_loss, forward_loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 38\u001b[0m, in \u001b[0;36mForwardModel.forward\u001b[0;34m(self, state_encoding_stack, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_encoding_stack, action):\n\u001b[0;32m---> 38\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate_encoding_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstate_encoding_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_model(input_tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 16 but got size 1 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Hopper-v2\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    episodes = 1000\n",
    "    batch_size = 64\n",
    "    buffer_size = 100000\n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.995\n",
    "\n",
    "    train_agent(env, episodes, batch_size, buffer_size, gamma, epsilon, epsilon_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
