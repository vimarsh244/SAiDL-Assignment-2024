{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get datasets: https://github.com/tkipf/gcn/tree/master/gcn/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:03<00:00,  1.01s/it]\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/hiv.zip\n",
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 95939.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:01<00:00, 38941.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygGraphPropPredDataset(name = \"ogbg-molhiv\", root = 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio-CE-LC.edges\n",
    "filename = 'bio-CE-LC'\n",
    "\n",
    "# Read the edge list file\n",
    "with open('data/bio-CE-LC.edges', 'r') as f:\n",
    "    edges = [line.strip().split() for line in f]\n",
    "\n",
    "# Extract nodes and edges from the edge list\n",
    "nodes = set()\n",
    "for edge in edges:\n",
    "    nodes.update(edge[:2])  # Extracting only source and target nodes\n",
    "\n",
    "# Generate or extract node features\n",
    "node_features = {node: [] for node in nodes}\n",
    "\n",
    "# Generate or extract target labels (assuming you want to predict edge weights)\n",
    "target_labels = {}\n",
    "\n",
    "for edge in edges:\n",
    "    source_node, target_node, edge_weight = edge\n",
    "    node_features[source_node].append(float(edge_weight))\n",
    "    node_features[target_node].append(float(edge_weight))\n",
    "    target_labels[(source_node, target_node)] = float(edge_weight)\n",
    "\n",
    "# Write node features to .x file\n",
    "with open('data/ind.'+filename+'.x', 'w') as f:\n",
    "    for node, features in node_features.items():\n",
    "        f.write(f\"{node} {' '.join(map(str, features))}\\n\")\n",
    "\n",
    "# Write target labels to .y file\n",
    "with open('data/ind.'+filename+'.y', 'w') as f:\n",
    "    for (source_node, target_node), label in target_labels.items():\n",
    "        f.write(f\"{source_node} {target_node} {label}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import sys\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features.todense(), sparse_to_tuple(features)\n",
    "\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=bool)\n",
    "\n",
    "def load_data(dataset_str): # {'pubmed', 'citeseer', 'cora'}\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    my_labels = np.where(labels==1)[1]\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_my_labels_mask = sample_mask(idx_train, my_labels.shape[0])\n",
    "    val_my_labels_mask = sample_mask(idx_val, my_labels.shape[0])\n",
    "    test_my_labels_mask = sample_mask(idx_test, my_labels.shape[0])\n",
    "    train_my_labels = my_labels[train_my_labels_mask]\n",
    "    val_my_labels = my_labels[val_my_labels_mask]\n",
    "    test_my_labels = my_labels[test_my_labels_mask]\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    print(adj.shape)\n",
    "    print(features.shape)\n",
    "    data_dict = {\n",
    "      'adj': adj,\n",
    "      'features': features,\n",
    "      'y_train': y_train,\n",
    "      'y_val': y_val,\n",
    "      'y_test': y_test,\n",
    "      'train_mask': train_mask,\n",
    "      'val_mask': val_mask,\n",
    "      'test_mask': test_mask,\n",
    "      'train_my_labels': train_my_labels,\n",
    "      'val_my_labels': val_my_labels,\n",
    "      'test_my_labels': test_my_labels,\n",
    "      'my_labels': my_labels\n",
    "    }\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attn_head(nn.Module):\n",
    "  def __init__(self, \n",
    "        in_channel, \n",
    "        out_sz, \n",
    "        bias_mat, \n",
    "        in_drop=0.0, \n",
    "        coef_drop=0.0, \n",
    "        activation=None,\n",
    "        residual=False):\n",
    "    super(Attn_head, self).__init__() \n",
    "    self.in_channel = in_channel\n",
    "    self.out_sz = out_sz \n",
    "    self.bias_mat = bias_mat\n",
    "    self.in_drop = in_drop\n",
    "    self.coef_drop = coef_drop\n",
    "    self.activation = activation\n",
    "    self.residual = residual\n",
    "    \n",
    "    self.conv1 = nn.Conv1d(self.in_channel, self.out_sz, 1)\n",
    "    self.conv2_1 = nn.Conv1d(self.out_sz, 1, 1)\n",
    "    self.conv2_2 = nn.Conv1d(self.out_sz, 1, 1)\n",
    "    self.leakyrelu = nn.LeakyReLU()\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "    self.in_dropout = nn.Dropout()\n",
    "    self.coef_dropout = nn.Dropout()\n",
    "    self.res_conv = nn.Conv1d(self.in_channel, self.out_sz, 1)\n",
    "  \n",
    "  def forward(self,x):\n",
    "    seq = x\n",
    "    if self.in_drop != 0.0:\n",
    "      seq = self.in_dropout(x)\n",
    "    seq_fts = self.conv1(seq)\n",
    "    f_1 = self.conv2_1(seq_fts)\n",
    "    f_2 = self.conv2_2(seq_fts)\n",
    "    logits = f_1 + torch.transpose(f_2, 2, 1)\n",
    "    logits = self.leakyrelu(logits)\n",
    "    coefs = self.softmax(logits + self.bias_mat)\n",
    "    if self.coef_drop !=0.0:\n",
    "      coefs = self.coef_dropout(coefs)\n",
    "    if self.in_dropout !=0.0:\n",
    "      seq_fts = self.in_dropout(seq_fts)\n",
    "    ret = torch.matmul(coefs, torch.transpose(seq_fts, 2, 1))\n",
    "    ret = torch.transpose(ret, 2, 1)\n",
    "    if self.residual:\n",
    "      if seq.shape[1] != ret.shape[1]:\n",
    "        ret = ret + self.res_conv(seq)\n",
    "      else:\n",
    "        ret = ret + seq\n",
    "    return self.activation(ret)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class GAT(nn.Module):\n",
    "  def __init__(self,\n",
    "      nb_classes, \n",
    "      nb_nodes, \n",
    "      attn_drop, \n",
    "      ffd_drop, \n",
    "      bias_mat, \n",
    "      hid_units, \n",
    "      n_heads, \n",
    "      residual=False):\n",
    "    super(GAT, self).__init__()  \n",
    "    self.nb_classes = nb_classes\n",
    "    self.nb_nodes = nb_nodes\n",
    "    self.attn_drop = attn_drop\n",
    "    self.ffd_drop = ffd_drop\n",
    "    self.bias_mat = bias_mat\n",
    "    self.hid_units = hid_units\n",
    "    self.n_heads = n_heads\n",
    "    self.residual = residual\n",
    "\n",
    "    self.attn1 = Attn_head(in_channel=1433, out_sz=self.hid_units[0],\n",
    "                bias_mat=self.bias_mat, in_drop=self.ffd_drop,\n",
    "                coef_drop=self.attn_drop, activation=nn.ELU(),\n",
    "                residual=self.residual)\n",
    "    self.attn2 = Attn_head(in_channel=64, out_sz=self.nb_classes,\n",
    "                bias_mat=self.bias_mat, in_drop=self.ffd_drop,\n",
    "                coef_drop=self.attn_drop, activation=nn.ELU(),\n",
    "                residual=self.residual)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    attns = []\n",
    "    for _ in range(self.n_heads[0]):\n",
    "      attns.append(self.attn1(x))\n",
    "    h_1 = torch.cat(attns, dim=1)\n",
    "    out = self.attn2(h_1)\n",
    "    logits = torch.transpose(out.view(self.nb_classes,-1), 1, 0)\n",
    "    logits = self.softmax(logits)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# data = load_data(\"cora\")\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbio-CE-LC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m adj \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madj\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m features \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 68\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dataset_str)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/ind.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_str, names[i]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m         objects\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpkl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         objects\u001b[38;5;241m.\u001b[39mappend(pkl\u001b[38;5;241m.\u001b[39mload(f))\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: unpickling stack underflow"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# data = load_data(\"cora\")\n",
    "\n",
    "data = load_data(\"bio-CE-LC\")\n",
    "\n",
    "adj = data['adj']\n",
    "features = data['features']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "y_test = data['y_test']\n",
    "train_mask = data['train_mask']\n",
    "val_mask = data['val_mask']\n",
    "test_mask = data['test_mask']\n",
    "train_my_labels = data['train_my_labels']\n",
    "val_my_labels = data['val_my_labels']\n",
    "test_my_labels = data['test_my_labels']\n",
    "my_labels = data['my_labels']\n",
    "\n",
    "features, spars = preprocess_features(features)\n",
    "\n",
    "nb_nodes = features.shape[0]\n",
    "ft_sizes = features.shape[1]\n",
    "nb_classes = my_labels.shape[0]\n",
    "\n",
    "adj = adj.todense()\n",
    "\n",
    "adj = adj[np.newaxis]\n",
    "features = features[np.newaxis]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]\n",
    "#train_mask = train_mask[np.newaxis]\n",
    "#val_mask = val_mask[np.newaxis]\n",
    "#test_mask = test_mask[np.newaxis]\n",
    "\n",
    "biases = torch.from_numpy(adj_to_bias(adj, [nb_nodes], nhood=1)).float().to(device)\n",
    "\n",
    "features = torch.from_numpy(features)\n",
    "#pytorch: [batch, features，nodes]，\n",
    "features = torch.transpose(features,2,1).to(device)\n",
    "\n",
    "hid_units=[8]\n",
    "n_heads=[8, 1]\n",
    "epochs = 2400\n",
    "lr = 0.01\n",
    "\n",
    "gat = GAT(nb_classes=nb_classes,\n",
    "      nb_nodes=nb_nodes, \n",
    "      attn_drop=0.0, \n",
    "      ffd_drop=0.0, \n",
    "      bias_mat=biases, \n",
    "      hid_units=hid_units, \n",
    "      n_heads=n_heads, \n",
    "      residual=False).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=gat.parameters(),lr=lr,betas=(0.9, 0.99))\n",
    "\n",
    "\n",
    "#y_train = torch.from_numpy(np.where(y_train==1)[2])\n",
    "#y_val = torch.from_numpy(np.where(y_val==1)[2])\n",
    "#y_test = torch.from_numpy(np.where(y_test==1)[2])\n",
    "train_my_labels = torch.from_numpy(train_my_labels).long().to(device)\n",
    "val_my_labels = torch.from_numpy(val_my_labels).long().to(device)\n",
    "test_my_labels = torch.from_numpy(test_my_labels).long().to(device)\n",
    "\n",
    "train_mask = np.where(train_mask == 1)[0]\n",
    "val_mask = np.where(val_mask == 1)[0]\n",
    "test_mask = np.where(test_mask == 1)[0]\n",
    "train_mask = torch.from_numpy(train_mask).to(device)\n",
    "val_mask = torch.from_numpy(val_mask).to(device)\n",
    "test_mask = torch.from_numpy(test_mask).to(device)\n",
    "\n",
    "print(\"number of labels in train set: \", len(train_my_labels))\n",
    "print(\"no of labels in val set: \", len(val_my_labels))\n",
    "print(\"no of labels in test set: \", len(test_my_labels))\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "  gat.train()\n",
    "  correct = 0\n",
    "  optimizer.zero_grad()\n",
    "  outputs = gat(features)\n",
    "  train_mask_outputs = torch.index_select(outputs, 0, train_mask)\n",
    "  #print(\"train_mask_outputs.shape:\",train_mask_outputs.shape)\n",
    "  #print(\"train_my_labels.shape[0]:\",train_my_labels.shape[0])\n",
    "  _, preds =torch.max(train_mask_outputs.data, 1)\n",
    "  loss = criterion(train_mask_outputs, train_my_labels)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  correct += torch.sum(preds == train_my_labels).to(torch.float32)\n",
    "  acc = correct / train_my_labels.shape[0]\n",
    "  return loss,acc\n",
    "\n",
    "\n",
    "def val():\n",
    "  gat.eval()\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    outputs = gat(features)\n",
    "    val_mask_outputs = torch.index_select(outputs, 0, val_mask)\n",
    "    #print(\"val_mask_outputs.shape:\",val_mask_outputs.shape)\n",
    "    #print(\"val_my_labels.shape[0]:\",val_my_labels.shape[0])\n",
    "    _, preds =torch.max(val_mask_outputs.data, 1)\n",
    "    loss = criterion(val_mask_outputs, val_my_labels)\n",
    "    correct += torch.sum(preds == val_my_labels).to(torch.float32)\n",
    "    acc = correct / val_my_labels.shape[0]\n",
    "  return loss,acc\n",
    "\n",
    "def test():\n",
    "  gat.eval()\n",
    "  with torch.no_grad():\n",
    "    correct = 0\n",
    "    outputs = gat(features)\n",
    "    test_mask_outputs = torch.index_select(outputs, 0, test_mask)\n",
    "    #print(\"test_mask_outputs.shape:\",test_mask_outputs.shape)\n",
    "    #print(\"val_my_labels.shape[0]:\",val_my_labels.shape[0])\n",
    "    _, preds =torch.max(test_mask_outputs.data, 1)\n",
    "    loss = criterion(test_mask_outputs, test_my_labels)\n",
    "    correct += torch.sum(preds == test_my_labels).to(torch.float32)\n",
    "    acc = correct / test_my_labels.shape[0]\n",
    "    print(\"TestLoss:{:.4f},TestAcc:{:.4f}\".format(loss,acc))\n",
    "  return loss,acc,test_mask_outputs.cpu().numpy(),test_my_labels.cpu().numpy()\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main():\n",
    "  train_loss_history = []\n",
    "  val_loss_history = []\n",
    "  train_acc_history = []\n",
    "  val_acc_history = []\n",
    "  for epoch in range(1,epochs+1):\n",
    "    train_loss,train_acc = train()\n",
    "    val_loss,val_acc = val()\n",
    "    print(\"epoch:{:03d},TrainLoss:{:.4f},TrainAcc:{:.4f},ValLoss:{:.4f},ValAcc:{:.4f}\"\n",
    "        .format(epoch,train_loss,train_acc,val_loss,val_acc))\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "  num_epochs = range(1, epochs + 1)\n",
    "  plt.plot(num_epochs, train_loss_history, 'b--')\n",
    "  plt.plot(num_epochs, val_loss_history, 'r-')\n",
    "  plt.title('Training and validation Loss ')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.legend([\"train_loss\", 'val_loss'])\n",
    "  plt.savefig(\"loss.png\")\n",
    "  plt.close()\n",
    "\n",
    "  plt.plot(num_epochs, train_acc_history, 'b--')\n",
    "  plt.plot(num_epochs, val_acc_history, 'r-')\n",
    "  plt.title('Training and validation Acc ')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(\"Acc\")\n",
    "  plt.legend(['train_acc','val_acc'])\n",
    "  plt.savefig(\"acc.png\")\n",
    "  plt.close()\n",
    "\n",
    "  _, _, test_data, test_labels = test()\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000) # TSNE降维，降到2\n",
    "  low_dim_embs = tsne.fit_transform(test_data)\n",
    "  plt.title('tsne result')\n",
    "  plt.scatter(low_dim_embs[:,0], low_dim_embs[:,1], marker='o', s=5, c=test_labels)\n",
    "  plt.savefig(\"tsne.png\")\n",
    "  plt.close()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
