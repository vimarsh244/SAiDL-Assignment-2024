{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vimarsh/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "squad_dataset = load_dataset('squad')\n",
    "print(squad_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing context with only space.\n",
    "\n",
    "def preprocess(example):\n",
    "    \n",
    "    out = {}\n",
    "\n",
    "    context_token = example['context'].strip().split(' ')\n",
    "    question_token = example['question'].strip().split(' ')\n",
    "    \n",
    "    out['context'] = context_token\n",
    "    out['question'] = question_token\n",
    "    \n",
    "    if 'answers' not in example:\n",
    "        return out\n",
    "    \n",
    "    answer_start = example['answers']['answer_start']\n",
    "    out['answers'] = []\n",
    "    for i, ans_st in enumerate(answer_start):\n",
    "        c_token_len = len(example['context'][:ans_st].strip().split(' '))\n",
    "        a_token_len = len(example['answers']['text'][i].strip().split(' '))\n",
    "        out['answers'].append({'start' : c_token_len, 'end' : c_token_len + a_token_len})\n",
    "    \n",
    "    return out\n",
    "\n",
    "# tokenizing conext with space and symbolic letter.\n",
    "\n",
    "def tokenizing1(text):\n",
    "    text = text.lower()\n",
    "    tokens = []\n",
    "    tokens_start = []\n",
    "    tokens_len = []\n",
    "    where_space = []\n",
    "    i, token_start, token_len = 0, 0, 0\n",
    "    \n",
    "    while i < len(text):\n",
    "        # is character alphbet?\n",
    "        if text[i].isalpha(): # alphabet\n",
    "            while True:\n",
    "                token_len += 1\n",
    "                if i + token_len >= len(text) or not text[i + token_len].isalpha():\n",
    "                    break\n",
    "            tokens.append(text[i:i+token_len])\n",
    "            tokens_len.append(token_len)\n",
    "            tokens_start.append(i)\n",
    "            i += token_len\n",
    "            token_len = 0\n",
    "        elif text[i] == ' ': # space\n",
    "            where_space.append(i)\n",
    "            i += 1\n",
    "        else: # symbolic char\n",
    "            tokens.append(text[i])\n",
    "            tokens_len.append(1)\n",
    "            tokens_start.append(i)\n",
    "            i += 1\n",
    "    return tokens, tokens_start, tokens_len, where_space\n",
    "\n",
    "def advanced_preprocess1(data):\n",
    "    \n",
    "    out = {}\n",
    "    tokens, tokens_start, tokens_len, where_space = tokenizing1(data['context'])\n",
    "    tokens_q, _, _, _ = tokenizing1(data['question'])\n",
    "    \n",
    "    answer_start = data['answers']['answer_start']\n",
    "    \n",
    "    out = {'context' : tokens, 'question' : tokens_q, 'tokens_start' : tokens_start, 'tokens_len' : tokens_len, 'where_space' : where_space}\n",
    "    \n",
    "    out['answers'] = []\n",
    "    \n",
    "    for i, ans in enumerate(answer_start):\n",
    "        start_index = tokens_start.index(ans)\n",
    "        tokens, _, _, _ = tokenizing1(data['answers']['text'][i])\n",
    "        end_index = start_index + len(tokens)\n",
    "        out['answers'].append({'start' : start_index, 'end' : end_index})\n",
    "    return out\n",
    "\n",
    "def token2string(context, tokens_start, tokens_len, tokens_answers, where_space, test = []):\n",
    "    \n",
    "    cont = ''\n",
    "\n",
    "    for i, token in enumerate(context):\n",
    "        if tokens_start[i] + tokens_len[i] in where_space:\n",
    "            cont += (token + ' ')\n",
    "        else:\n",
    "            cont += token\n",
    "    \n",
    "    if len(test) > 0 :\n",
    "            start_index = tokens_start[test[0]] if test[0] < len(context) else tokens_start[-1]\n",
    "            end_index = tokens_start[test[1]] if test[1] < len(context) else tokens_start[-1]\n",
    "    else:\n",
    "        try:\n",
    "            tokens_answers_start = tokens_answers['start']\n",
    "            tokens_answers_end = tokens_answers['end']\n",
    "            start_index = tokens_start[tokens_answers_start]\n",
    "            end_index = tokens_start[tokens_answers_end-1] + tokens_len[tokens_answers_end -1]\n",
    "        except:\n",
    "             pass\n",
    "    \n",
    "    ans = cont[start_index : end_index]\n",
    "    return cont, ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataloader \n",
    "\n",
    "\n",
    "def preprocessing(squad_dataset):\n",
    "\ttrain_data = []\n",
    "\tdel_train_data_index = []\n",
    "\tcount = 0\n",
    "\tfor i, data in enumerate(squad_dataset['train']):\n",
    "\t    try:\n",
    "\t        train_data.append(advanced_preprocess1(data))\n",
    "\t    except:\n",
    "\t        train_data.append([])\n",
    "\t        del_train_data_index.append(i)\n",
    "\t        count+=1\n",
    "\t        \n",
    "\n",
    "\tvalid_data = []\n",
    "\tdel_valid_data_index = []\n",
    "\tcount = 0\n",
    "\tfor i, data in enumerate(squad_dataset['validation']):\n",
    "\t    try:\n",
    "\t        valid_data.append(advanced_preprocess1(data))\n",
    "\t    except:\n",
    "\t        valid_data.append([])\n",
    "\t        del_valid_data_index.append(i)\n",
    "\t        count += 1\n",
    "\n",
    "\treturn train_data, del_train_data_index, valid_data, del_valid_data_index\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, del_train_data_index, valid_data, del_valid_data_index = preprocessing(squad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make batch . \n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def make_batch(data, batch_size = 64, index = [], random = True, question = False):\n",
    "    print(data)\n",
    "    data = np.array(data)\n",
    "    if random:\n",
    "        indice = np.random.choice(len(data), batch_size, replace = False)\n",
    "        for i, idx in enumerate(indice):\n",
    "            if idx in del_train_data_index:\n",
    "                indice[i] = 0\n",
    "        data_batch = data[indice]\n",
    "        \n",
    "    else:\n",
    "        for i,idx in enumerate(index):\n",
    "            if idx in del_valid_data_index:\n",
    "                index[i] = 0\n",
    "        data_batch = data[index]\n",
    "\n",
    "    context_max_len = 0\n",
    "    question_max_len = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if question:\n",
    "            context_max_len = max(context_max_len, len(data_batch[i]['context']) + len(data_batch[i]['question']))\n",
    "        else:\n",
    "            context_max_len = max(context_max_len, len(data_batch[i]['context']))\n",
    "    \n",
    "    context_batch = []\n",
    "    answer_start_batch = []\n",
    "    answer_end_batch = []\n",
    "    context_mask = []\n",
    "    mask_loc = []\n",
    "    \n",
    "    for i, d in enumerate(data_batch):\n",
    "        if d == []:\n",
    "            continue\n",
    "            \n",
    "        context, questions, answer = wordToid(d)\n",
    "        \n",
    "        if question:\n",
    "            context = np.concatenate([context, questions])\n",
    "        \n",
    "        context_len = len(context)\n",
    "        context_padding = np.zeros(context_max_len - len(context))\n",
    "        context = np.concatenate([context, context_padding])\n",
    "        context_batch.append(context)\n",
    "        \n",
    "        for answers in data_batch[i]['answers']:\n",
    "            answer_start_batch.append(answers['start'])\n",
    "            answer_end_batch.append(answers['end'])\n",
    "        \n",
    "        context_mask.append(np.concatenate([np.ones(context_len), np.zeros(len(context_padding))], axis = 0))\n",
    "        mask_loc.append(context_len)\n",
    "        \n",
    "    return torch.LongTensor(context_batch), torch.LongTensor(answer_start_batch), torch.LongTensor(answer_end_batch), torch.LongTensor(context_mask), torch.LongTensor(mask_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# word2id, id2word\n",
    "\n",
    "def make_dict(train_data, del_train_data_index):\n",
    "\n",
    "\tword2id = {}\n",
    "\ttokens = []\n",
    "\tdel_idx = []\n",
    "\tfor i in range(len(train_data)):\n",
    "\t    if i in del_train_data_index:\n",
    "\t        continue\n",
    "\t    context_tokens = train_data[i]['context']\n",
    "\t    question_tokens = train_data[i]['question']\n",
    "\t    tokens.extend(context_tokens)\n",
    "\t    tokens.extend(question_tokens)\n",
    "\t    if not train_data[i]['answers']:\n",
    "\t        del_idx.extend([i])\n",
    "\n",
    "\tvocab = ['UNK'] + list(set(tokens))\n",
    "\t    \n",
    "\tword2id = {word : id_ for id_ , word in enumerate(vocab)}\n",
    "\tid2word = {id_ : word for word, id_ in word2id.items()}\n",
    "\n",
    "\treturn word2id, id2word\n",
    "\n",
    "def wordToid(data):\n",
    "    context = data['context']\n",
    "    question = data['question']\n",
    "    \n",
    "    context = [word2id[word] if word in word2id else 0 for word in context]\n",
    "    question = [word2id[word] if word in word2id else 0 for word in question]\n",
    "    \n",
    "    answer = []\n",
    "    \n",
    "    for dic in data['answers']:\n",
    "        start = dic['start']\n",
    "        end = dic['end']\n",
    "        answer.append(context[start:end])\n",
    "    return context, question, answer\n",
    "\n",
    "\n",
    "\n",
    "def valid_answers(data_index, index = []):\n",
    "    data = valid_data[data_index]\n",
    "    context = data['context']\n",
    "    tokens_start = data['tokens_start']\n",
    "    tokens_len = data['tokens_len']\n",
    "    where_space = data['where_space']\n",
    "    \n",
    "    if index[-1] > len(context):\n",
    "        index[-1] = len(context)\n",
    "    \n",
    "    ans_tokens = context[index[0]:index[1]]\n",
    "\n",
    "    ans = ''\n",
    "    for i, token in enumerate(ans_tokens):\n",
    "        if tokens_start[index[0] + i] + tokens_len[index[0] + i] in where_space:\n",
    "            ans += (token + ' ')\n",
    "        else:\n",
    "            ans += token\n",
    "    \n",
    "    return ans.strip().lower()\n",
    "    \n",
    "def id2word_answer(data_index, start, end):\n",
    "    ### valid 비어있는지 확인하기 !!!\n",
    "    \n",
    "    data = valid_data[data_index] \n",
    "    origin_data = squad_dataset['validation'][data_index]\n",
    "    \n",
    "    ans = valid_answers(data_index, [start,end])\n",
    "    \n",
    "    for i, a in enumerate(origin_data['answers']['text']):\n",
    "        origin_data['answers']['text'][i] = a.lower()\n",
    "    \n",
    "    prediction_ = {'prediction_text': ans, 'id': origin_data['id']}\n",
    "    reference = {'answers' : origin_data['answers'], 'id': origin_data['id']}\n",
    "    \n",
    "    return prediction_, reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - layer lstm with drop out using pytorch lstm\n",
    "class LSTM_dropout(nn.Module):\n",
    "    def __init__(self, voca_size, embed_dim, hidden_dim, dropout):\n",
    "        super(LSTM_dropout, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(voca_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = hidden_dim, num_layers = 1,\n",
    "                            batch_first = True, dropout = dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_lin = nn.Linear(hidden_dim, 2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        embeded = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        h_t, c_t = self.init_state(batch_size)\n",
    "        \n",
    "        outputs, (hidden, cell_state)= self.lstm(embeded, (h_t, c_t))\n",
    "        \n",
    "        mask = mask.unsqueeze(-1)\n",
    "         # batch * seq_len * hidden_dim\n",
    "        \n",
    "        outputs = self.out_lin(outputs) # batch * seq_len * 2\n",
    "        \n",
    "        outputs_masked = outputs * mask\n",
    "        \n",
    "        out_start = outputs_masked[:,:,0]\n",
    "        out_end = outputs_masked[:,:,1]\n",
    "        \n",
    "        return out_start, out_end\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim).to(device), torch.zeros(1, batch_size, self.hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(data, model, criterion, optimizer, batch_size = 128, num_iter = 30000, question = False, attention = False):\n",
    "    device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    loss_ = 0.\n",
    "    acc_, start_acc, end_acc, val_score = 0, 0, 0, 0\n",
    "    \n",
    "    for i in tqdm(range(num_iter)):\n",
    "        context, answer_start, answer_end, mask, loc = make_batch(data, batch_size, random = True, question = question)\n",
    "        context, answer_start, answer_end, mask, loc = context.to(device), answer_start.to(device), answer_end.to(device), mask.to(device), loc.to(device)\n",
    "#         context, answer_start, answer_end = context.to(device), answer_start.to(device), answer_end.to(device), #mask.to(device), loc.to(device) ###\n",
    "        \n",
    "        if attention:\n",
    "            start, end = model(context, mask, loc)\n",
    "        else:\n",
    "            start, end = model(context, mask)\n",
    "\n",
    "        \n",
    "        loss_start = criterion(start, answer_start)\n",
    "        loss_end = criterion(end, answer_end-1)\n",
    "        \n",
    "        loss = loss_start/2  + loss_end/2\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss\n",
    "        loss_ += loss.detach().cpu()\n",
    "        \n",
    "        # acc\n",
    "        start_acc_ = (start.argmax(dim = -1) == answer_start).detach()\n",
    "        end_acc_ = (end.argmax(dim = -1) == (answer_end -1)).detach()\n",
    "        start_acc += start_acc_.sum().item()*1./batch_size\n",
    "        end_acc += end_acc_.sum().item()*1./batch_size\n",
    "        acc_ += (start_acc_ & end_acc_).sum().item()*1./batch_size\n",
    "                \n",
    "        if i % 1000 == 999 :\n",
    "            print(f'{i + 1 : d}th iters >> loss = {loss_/1000:.4f}, start_acc = {start_acc/1000 * 100 : .4f}, end_acc = {end_acc/1000 * 100 : .4f}, acc = {acc_/1000 * 100 : .4f}')\n",
    "            loss_, start_acc, end_acc, acc_ = 0., 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                cur_val_score = validation(valid_data, model = model, batch_size = 128, question = question, attention = attention)\n",
    "#                 if attention:\n",
    "#                     if cur_val_score < val_score:\n",
    "#                         optimizer.factor *= 0.8\n",
    "#                 val_score = cur_val_score\n",
    "                model.train()\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(data, model, batch_size = 128, question = False, attention = False):\n",
    "    device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "\n",
    "    for i in range(len(data)//batch_size):\n",
    "        c, a_s, a_e, m, l= make_batch(data, batch_size = batch_size, index = np.arange(i*batch_size, (i+1)*batch_size), random = False, question = question)\n",
    "        c, a_s, a_e, m, l = c.to(device), a_s.to(device), a_e.to(device), m.to(device), l.to(device)\n",
    "        \n",
    "        if attention:\n",
    "            start, end = model(c,m,l)\n",
    "        else:\n",
    "            start, end = model(c, m) # batch_size * seq_len\n",
    "        \n",
    "        start_index = start.argmax(dim = 1).detach()\n",
    "        end_index = end.argmax(dim = 1).detach()\n",
    "        end_index += 1\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            if i * batch_size + j in del_valid_data_index:\n",
    "                continue\n",
    "            if start_index[j] >= end_index[j]:\n",
    "                continue\n",
    "            pred, refer = id2word_answer(i * batch_size + j, start_index[j], end_index[j])\n",
    "            predictions.append(pred)\n",
    "            references.append(refer)\n",
    "\n",
    "    results = squad_metric.compute(predictions = predictions, references = references)\n",
    "    print('validation score : ', results)\n",
    "    return results['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vimarsh/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = (len(make_dict(train_data, del_train_data_index)[0]))\n",
    "embed_dim = 100\n",
    "hidden_dim = 128\n",
    "dropout = 0.5\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTM_dropout(vocab_size, embed_dim, hidden_dim, dropout)\n",
    "\n",
    "import torch.optim as optim\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "# train_data, val_data = train_test_split(train_data, test_size=0.1)\n",
    "\n",
    "val_data = valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Train the model\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i in tqdm(range(0, len(train_data), batch_size)):\n",
    "#         batch_data = train_data[i:i+batch_size]\n",
    "#         inputs = [torch.tensor(example['context']) for example in batch_data]\n",
    "#         labels = [torch.tensor([example['start'], example['end']]) for example in batch_data]\n",
    "#         inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "#         labels = torch.stack(labels)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {running_loss}\")\n",
    "\n",
    "# # Evaluate the model on validation set\n",
    "# model.eval()\n",
    "# val_loss = 0.0\n",
    "# with torch.no_grad():\n",
    "#     for i in tqdm(range(0, len(val_data), batch_size)):\n",
    "#         batch_data = val_data[i:i+batch_size]\n",
    "#         inputs = [torch.tensor(example['context']) for example in batch_data]\n",
    "#         labels = [torch.tensor([example['start'], example['end']]) for example in batch_data]\n",
    "#         inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "#         labels = torch.stack(labels)\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         val_loss += loss.item()\n",
    "# print(f\"Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:04<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train(train_data, model, criterion, optimizer, batch_size=batch_size, num_iter=num_epochs, question=False)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "validation(valid_data, model=model, batch_size=batch_size, question=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
